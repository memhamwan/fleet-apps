helm:
  repo: https://prometheus-community.github.io/helm-charts
  chart: prometheus
  version: 14.0.0
  values:
    server:
      image:
        tag: v2.27.1
      baseURL: "https://prometheus.k8s.leb.memhamwan.net"
      persistentVolume:
        enabled: false
      ingress:
        enabled: true
        hosts:
          - "prometheus.k8s.leb.memhamwan.net"
        annotations:
          kubernetes.io/tls-acme: "true"
          kubernetes.io/ingress.class: nginx
          cert-manager.io/cluster-issuer: letsencrypt-prod
        tls:
          - secretName: prometheus-tls
            hosts:
              - prometheus.k8s.leb.memhamwan.net
    alertmanager:
      enabled: true
      baseURL: "https://alertmanager.k8s.leb.memhamwan.net"
      configFromSecret: "alertmanager-config-secret"
      persistentVolume:
        enabled: false
      ingress:
        enabled: true
        hosts:
          - "alertmanager.k8s.leb.memhamwan.net"
        annotations:
          kubernetes.io/tls-acme: "true"
          kubernetes.io/ingress.class: nginx
          cert-manager.io/cluster-issuer: letsencrypt-prod
        tls:
          - secretName: alertmanager-tls
            hosts:
              - alertmanager.k8s.leb.memhamwan.net
    grafana:
      enabled: true
      adminUser: "hamwan"
      adminPassword: "hamwan"
      ingress:
        enabled: true
        hosts:
          - "grafana.k8s.leb.memhamwan.net"
        annotations:
          kubernetes.io/tls-acme: "true"
          kubernetes.io/ingress.class: nginx
          cert-manager.io/cluster-issuer: letsencrypt-prod
        tls:
          - secretName: grafana-tls
            hosts:
              - grafana.k8s.leb.memhamwan.net
    alertmanagerFiles:
    serverFiles:

      ## Alerts configuration
      ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
      alerting_rules.yml:
        groups:
          - name: Network
            rules:
              - alert: SiteToSiteLoss
                expr: instance:smokeping_probe_success:ratio1m < 0.99
                for: 2m
                labels:
                  severity: page
                annotations:
                  description: '{{ $labels.job }} to {{ $labels.host }} has shown lower than 99% success rate for 2 min'
                  summary: 'Packet loss to {{ $labels.host }}'
          - name: Instances
            rules:
              - alert: InstanceDown
                expr: up == 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
                  summary: 'Instance {{ $labels.instance }} down'
              - alert: RpiHighCPUTemp
                expr: rpi_temperature > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Raspberry Pi {{ $labels.instance }} of job {{ $labels.job }} has had a high CPU temperature > 80c for 5 minutes.'
                  summary: 'Raspberry Pi {{ $labels.instance }} high CPU temperature'
              - alert: HostOutOfMemory
                expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: Host out of memory (instance {{ $labels.instance }})
                  description: 'Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}'
              - alert: HostHighCpuLoad
                expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
                for: 0m
                labels:
                  severity: warning
                annotations:
                  summary: Host high CPU load (instance {{ $labels.instance }})
                  description: 'CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}'
              - alert: HostSwapIsFillingUp
                expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: Host swap is filling up (instance {{ $labels.instance }})
                  description: 'Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}'
              - alert: HostOutOfDiskSpace
                expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: Host out of disk space (instance {{ $labels.instance }})
                  description: 'Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}'
      ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
      alerts: {}

      ## Records configuration
      ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
      recording_rules.yml:
        groups:
          - name: Smokeping
            interval: 30s
            rules:
              - record: instance:smokeping_probe_success:ratio1m
                expr: increase(smokeping_response_duration_seconds_count[1m]) / increase(smokeping_requests_total[1m])
              - record: instance:smokeping_response_duration_seconds:q50
                expr: histogram_quantile(0.50, rate(smokeping_response_duration_seconds_bucket[1m]))
              - record: instance:smokeping_response_duration_seconds:q90
                expr: histogram_quantile(0.90, rate(smokeping_response_duration_seconds_bucket[1m]))
              - record: instance:smokeping_response_duration_seconds:q99
                expr: histogram_quantile(0.99, rate(smokeping_response_duration_seconds_bucket[1m]))
      ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
      rules: {}

      prometheus.yml:
        rule_files:
          - /etc/config/recording_rules.yml
          - /etc/config/alerting_rules.yml
          - /etc/config/rules
          - /etc/config/alerts

        scrape_configs:
          - job_name: 'adsb'
            static_configs:
              - targets:
                  - adsb.sco.memhamwan.net:9105
            metrics_path: /metrics
          - job_name: 'smokeping'
            static_configs:
              - targets:
                  - smokeping-exporter.k8s.leb.memhamwan.net
            metrics_path: /metrics
            scrape_interval: 15s
          - job_name: 'snmp-routers'
            static_configs:
              - targets:
                  - ap.mno.memhamwan.net
                  - azo.leb.memhamwan.net
                  - crw.leb.memhamwan.net
                  - hil.leb.memhamwan.net
                  - hil.mno.memhamwan.net
                  - leb.azo.memhamwan.net
                  - leb.hil.memhamwan.net
                  - leb.mno.memhamwan.net
                  - leb.ret.memhamwan.net
                  - leb.sco.memhamwan.net
                  - mno.hil.memhamwan.net
                  - mno.leb.memhamwan.net
                  - omn1.leb.memhamwan.net
                  - omn2.azo.memhamwan.net
                  - r1.azo.memhamwan.net
                  - r1.hil.memhamwan.net
                  - r1.leb.memhamwan.net
                  - r1.mno.memhamwan.net
                  - r1.sco.memhamwan.net
                  - r2.azo.memhamwan.net
                  - r2.leb.memhamwan.net
                  - r2.mno.memhamwan.net
                  - r3.azo.memhamwan.net
                  - r4.azo.memhamwan.net
                  - ret.leb.memhamwan.net
                  - sco.leb.memhamwan.net
                  - sec1.hil.memhamwan.net
                  - sec1.mno.memhamwan.net
                  - sec1.sco.memhamwan.net
                  - sec2.hil.memhamwan.net
                  - sec2.mno.memhamwan.net
                  - sec2.sco.memhamwan.net
                  - sec3.hil.memhamwan.net
                  - sec3.mno.memhamwan.net
                  - sec3.sco.memhamwan.net
                  - leb.crw.memhamwan.net
                  - r1.crw.memhamwan.net
                  - r2.crw.memhamwan.net
                  - r3.crw.memhamwan.net
                  - omn1.crw.memhamwan.net
                  - omn2.crw.memhamwan.net
            metrics_path: /snmp
            params:
              module: [mikrotik]
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: snmp-exporter.k8s.leb.memhamwan.net
          - job_name: 'snmp-apc'
            static_configs:
              - targets:
                  - ups.mno.memhamwan.net
                  - ups.sco.memhamwan.net
                  - ups.azo.memhamwan.net
            metrics_path: /snmp
            params:
              module: [apcups]
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: snmp-exporter.k8s.leb.memhamwan.net
          - job_name: 'snmp-ifmib'
            static_configs:
              - targets:
                  - pdu.crw.memhamwan.net
            metrics_path: /snmp
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: snmp-exporter.k8s.leb.memhamwan.net
          - job_name: prometheus
            static_configs:
              - targets:
                  - localhost:9090
          - job_name: 'blackbox'
            metrics_path: /probe
            params:
              module: [http_2xx]  # Look for a HTTP 200 response.
            static_configs:
              - targets:
                  - http://clients1.google.com/generate_204
                  - http://www.wb4kog.com
                  - http://cam1.azo.memhamwan.net
                  - http://ais.azo.memhamwan.net
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: blackbox-exporter.k8s.leb.memhamwan.net  # The blackbox exporter's real hostname:port.
          - job_name: 'allstar'
            metrics_path: /probe
            params:
              module: [tcp_connect]
            static_configs:
              - targets:
                  - allstar.leb.memhamwan.net:5038
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: blackbox-exporter.k8s.leb.memhamwan.net  # The blackbox exporter's real hostname:port.
          - job_name: 'ip-cameras'
            metrics_path: /probe
            params:
              module: [tcp_connect]
            static_configs:
              - targets:
                  - cam1.azo.memhamwan.net:554
                  - cam1.hil.memhamwan.net:1935
                  - cam1.crw.memhamwan.net:554
                  - cam2.crw.memhamwan.net:554
                  - cam1.sco.memhamwan.net:554
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: blackbox-exporter.k8s.leb.memhamwan.net  # The blackbox exporter's real hostname:port.
          - job_name: 'node_exporter'
            static_configs:
              - targets:
                  - 'ntp.leb.memhamwan.net:9100'
                  - 'stream.sco.memhamwan.net:9100'
                  - 'dns.leb.memhamwan.net:9100'
                  - 'adsb.sco.memhamwan.net:9100'

          # A scrape configuration for running Prometheus on a Kubernetes cluster.
          # This uses separate scrape configs for cluster components (i.e. API server, node)
          # and services to allow each to use different authentication configs.
          #
          # Kubernetes labels will be added as Prometheus labels on metrics via the
          # `labelmap` relabeling action.

          # Scrape config for API servers.
          #
          # Kubernetes exposes API servers as endpoints to the default/kubernetes
          # service so this uses `endpoints` role and uses relabelling to only keep
          # the endpoints associated with the default/kubernetes service using the
          # default named port `https`. This works for single API server deployments as
          # well as HA API server deployments.
          - job_name: 'kubernetes-apiservers'

            kubernetes_sd_configs:
              - role: endpoints

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            # Keep only the default/kubernetes service endpoints for the https port. This
            # will add targets for each API server which Kubernetes adds an endpoint to
            # the default/kubernetes service.
            relabel_configs:
              - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                action: keep
                regex: default;kubernetes;https

          - job_name: 'kubernetes-nodes'

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
              - role: node

            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/$1/proxy/metrics


          - job_name: 'kubernetes-nodes-cadvisor'

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
              - role: node

            # This configuration will work only on kubelet 1.7.3+
            # As the scrape endpoints for cAdvisor have changed
            # if you are using older version you need to change the replacement to
            # replacement: /api/v1/nodes/$1:4194/proxy/metrics
            # more info here https://github.com/coreos/prometheus-operator/issues/633
            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

          # Scrape config for service endpoints.
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: If the metrics are exposed on a different port to the
          # service then set this appropriately.
          - job_name: 'kubernetes-service-endpoints'

            kubernetes_sd_configs:
              - role: endpoints

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: kubernetes_name
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node

          # Scrape config for slow service endpoints; same as above, but with a larger
          # timeout and a larger interval
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: If the metrics are exposed on a different port to the
          # service then set this appropriately.
          - job_name: 'kubernetes-service-endpoints-slow'

            scrape_interval: 5m
            scrape_timeout: 30s

            kubernetes_sd_configs:
              - role: endpoints

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: kubernetes_name
              - source_labels: [__meta_kubernetes_pod_node_name]
                action: replace
                target_label: kubernetes_node

          - job_name: 'prometheus-pushgateway'
            honor_labels: true

            kubernetes_sd_configs:
              - role: service

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                action: keep
                regex: pushgateway

          # Example scrape config for probing services via the Blackbox Exporter.
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/probe`: Only probe services that have a value of `true`
          - job_name: 'kubernetes-services'

            metrics_path: /probe
            params:
              module: [http_2xx]

            kubernetes_sd_configs:
              - role: service

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                action: keep
                regex: true
              - source_labels: [__address__]
                target_label: __param_target
              - target_label: __address__
                replacement: blackbox
              - source_labels: [__param_target]
                target_label: instance
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                target_label: kubernetes_name

          # Example scrape config for pods
          #
          # The relabeling allows the actual pod scrape endpoint to be configured via the
          # following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
          - job_name: 'kubernetes-pods'

            kubernetes_sd_configs:
              - role: pod

            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                action: replace
                regex: (https?)
                target_label: __scheme__
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name
              - source_labels: [__meta_kubernetes_pod_phase]
                regex: Pending|Succeeded|Failed
                action: drop

          # Example Scrape config for pods which should be scraped slower. An useful example
          # would be stackriver-exporter which queries an API on every scrape of the pod
          #
          # The relabeling allows the actual pod scrape endpoint to be configured via the
          # following annotations:
          #
          # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
          - job_name: 'kubernetes-pods-slow'

            scrape_interval: 5m
            scrape_timeout: 30s

            kubernetes_sd_configs:
              - role: pod

            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                action: replace
                regex: (https?)
                target_label: __scheme__
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name
              - source_labels: [__meta_kubernetes_pod_phase]
                regex: Pending|Succeeded|Failed
                action: drop
